import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Reading data from CSV
df = pd.read_csv('Covariance.csv')
print('data:')
print(df)

# Standardize the data
scaler = StandardScaler()
standardized_data = scaler.fit_transform(df)

# Convert standardized data back to DataFrame
df_standardized = pd.DataFrame(standardized_data, columns=df.columns)
print("\nStandardized Data:")
print(df_standardized)

# Calculating the standard deviation
std_dev_x = df_standardized['x'].std()
std_dev_y = df_standardized['y'].std()
print("\nStandard Deviation of x (s_x):", std_dev_x.round(3))
print("Standard Deviation of y (s_y):", std_dev_y.round(3))

# Calculating the covariance matrix
covariance_matrix = df.cov()

# Perform eigenvalue decomposition from covariance matrix
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

# Calculating Pearson correlation matrix 
corr_matrix =df.corr()

#In a Pearson correlation matrix, the terms (1,1) and (2,2)- 
# which represent the correlation of a variable with itself â€” are always 1.0. 

# Displaying the results
print("Covariance Matrix:"), print(covariance_matrix.round(3))
print("\nEigenvalues:"), print(eigenvalues.round(3))

# Creating a diagonal matrix with the eigenvalues
diagonal_matrix = np.diag(eigenvalues)

print("Diagonal Matrix:"), print(diagonal_matrix.round(3))
print("\nEigenvectors:"), print(eigenvectors.round(3))
print("\nPearson_correlation_matrix :"), print(corr_matrix .round(3))

# Identifying the principal component (eigenvector with the largest eigenvalue)
principal_component = eigenvectors[:, np.argmax(eigenvalues)]
print("principal_component:"), print(principal_component.round(3))

# Projecting the data onto the principal component
projected_data = df @ principal_component
print('\nprojected_data:'), print(projected_data.round(3))
# Plotting the original and transformed data
plt.figure(figsize=(10, 5))

# Original data plot
plt.subplot(1, 2, 1)
plt.scatter(df['x'], df['y'], color='blue')  # Corrected indexing
plt.title('Original Data')
plt.xlabel('X'), plt.ylabel('Y')


# Add the line of principal direction
line_x = np.array([np.min(df['x']), np.max(df['x'])])  # Corrected indexing
line_y = line_x * (principal_component[1] / principal_component[0])
plt.plot(line_x, line_y, color='green', linestyle='--')

# Projected data plot
plt.subplot(1, 2, 2)
plt.scatter(projected_data, np.zeros_like(projected_data), color='red')
plt.title('Projected Data (1D)')
plt.xlabel('Projected X'), plt.yticks([])

plt.tight_layout()
plt.show()

#check the eigenvalue decomposition
A=eigenvectors@diagonal_matrix@eigenvectors.T
print(A.round(3))

# Origin of the eigenvectors
origin = [0, 0] # Using (0,0) as the origin for simplicity

# Plotting the eigenvectors
for vec in eigenvectors.T:
  plt.quiver(*origin, *vec, color='r', scale=1, scale_units='xy', angles='xy')

plt.xlim(-1, 1), plt.ylim(0, 2)
plt.xlabel('X'), plt.ylabel('Y')
plt.title('Data Points and Eigenvector Directions')
plt.axhline(0, color='black',linewidth=0.5)
plt.axvline(0, color='black',linewidth=0.5)
plt.grid(color='gray', linestyle='--', linewidth=0.5)
plt.legend()
plt.show()

