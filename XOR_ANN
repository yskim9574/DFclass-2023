# XOR gate 문제
# sigmoid 함수만 사용
# 입력 4가지 상태, 입력층 2개 뉴런, 1개의 은닉층에 2개의 뉴런, 출럭층에 1개의 뉴런
# 표시는 주석임
# 이하에는 프로그램의 중요한 부분만을 나타내었다.
import numpy as np 
import matplotlib.pyplot as plt

def relu(x):
    return np.maximum(0,x)

def relu_derivative(x):
    return np.where(x>0,1,0)

def sigmoid (x):
    return 1/(1 + np.exp(-x))
def sigmoid_derivative(x):
    return x * (1 - x)

# XOR 문제 입력 데이타 셋트
inputs = np.array([[0,0],[0,1],[1,0],[1,1]])
outputs = np.array([[0],[1],[1],[0]])

inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1
# 가중값과 바이어스값 지정
w1=np.array([[0.1, 0.2], [0.3,0.4]])
b1=np.array([[0.1, 0.2]])
w2=np.array([[0.5], [0.6]])
b2=np.array([[0.3]])

# 가중값과 바이어스값 랜덤 설정
#np.random.seed(0)
#w1 = np.random.randn(inputLayerNeurons,hiddenLayerNeurons)
#b1 =np.zeros((1,hiddenLayerNeurons))
#w2 = np.random.randn(hiddenLayerNeurons,outputLayerNeurons)
#b2 = np.zeros((1,outputLayerNeurons))

epochs = 10000
lr = 0.1  #학습률
losses=[]

# 학습 알고리즘
for epoch in range(epochs):

 # 순방향 계산
  hidden_layer_input = np.dot(inputs,w1) + b1  #u=x*w1+b1
  hidden_layer_output = relu(hidden_layer_input) #h=sig(u)
  output_layer_input = np.dot(hidden_layer_output,w2) + b2 #v=h*w2+b2
  predicted_output = sigmoid(output_layer_input) #O=sig(v)

# 역방향 계산(역전파)
  error = outputs - predicted_output #e= {dE/dO} = -(O-t)
  d_predicted_output = error * sigmoid_derivative(predicted_output) #{dE/dO}*{dO/dv}  
  error_hidden_layer = d_predicted_output.dot(w2.T)
#(dE/dO)*(dO/dv)*(dv/dh)=(dE/dO)*(dO/dv)*(w2^T), dv/dh=w1
  d_hidden_layer = error_hidden_layer * relu_derivative(hidden_layer_output) 
#{(dE/dO)*(dO/dv)*(w2^T)}*{dh/du}

# 가중값과 바이어스값 갱신
  w2 += hidden_layer_output.T.dot(d_predicted_output) * lr 
#w2=w2-alpha*dE/dw2=w2-alpha*{(dE/dO}*(dO/dv))*(dv/dw2)
  b2 += np.sum(d_predicted_output,axis=0,keepdims=True) * lr
  w1 += inputs.T.dot(d_hidden_layer) * lr
#w1=w1-alpha*dE/dw1=w1-alpha*{(dE/dO)*(dO/dv))*(dv/dh)}*{(dh/du)*(du/dw1)}, du/dw1=x
  b1 += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr

#손실 계산(MSE-Mean Squared Error)
  loss=np.mean(np.square(error))
  losses.append(loss)

# 결과 출력
# Plot the loss values
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs. Epoch')
plt.show()

# Test the model for the input [1, 0]
test_input = np.array([[1, 0]])
hidden_layer_output = relu(np.dot(test_input, W1) + b1)
predicted_output = sigmoid(np.dot(hidden_layer_output, W2) + b2)
print("output after 10000 epoche:" , predicted_output)

print("Final hidden weights: ",end='')
print(*hidden_weights)
print("Final hidden bias: ",end='')
print(*hidden_bias)
print("Final output weights: ",end='')
print(*output_weights)
print("Final output bias: ",end='')
print(*output_bias)
print("\nOutput from neural network after 10,000 epochs: ",end='')
print(*predicted_output)
